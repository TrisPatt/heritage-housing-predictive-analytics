{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **Data Cleaning Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Evaluate missing data\n",
    "* Clean Data\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/house_prices_records.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Generate cleaned Test and Train sets \n",
    "* Data cleaning pipeline\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "* Drop ['EnclosedPorch', 'WoodDecksF']\n",
    "* Cateogorical imputer- ['GarageFinish', 'BsmtFinType1']\n",
    "* Median imputer- ['2ndFlrSF', 'BedroomAbvGr', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOGIGS-uz3i2"
   },
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MWW8E7lz3i7"
   },
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_xPk_Ijz3i-"
   },
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f\"inputs/datasets/raw/house-price-20211124T154130Z-001/house-price/house_prices_records.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find the values with missing data, store in an array and create a Pandas profiling report.\n",
    "This shows 9 variables with missing data, the total of each can be seen in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Missing Data Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom function to display missing data levels in a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                   \"DataType\": df.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check missing data levels for the collected dataset. This shows the number of missing rows and the percentage of missing values per feature as well as data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that EnclosedPorch and WoodDeckSF exhibit high levels of missing data, suggesting that these features may have limited predictive value for estimating the sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing With Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyse and visualize the effects of data cleaning methods on specified variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
    "    \"\"\"\n",
    "    Analyse and visualize the distribution effect of data cleaning on variables.\n",
    "\n",
    "    Parameters:\n",
    "    - df_original: Original dataset (DataFrame)\n",
    "    - df_cleaned: Cleaned dataset (DataFrame)\n",
    "    - variables_applied_with_method: List of variables to analyse\n",
    "    \"\"\"      \n",
    "    flag_count = 1  \n",
    "\n",
    "    categorical_vars = [var for var in variables_applied_with_method if df_original[var].dtype == 'object']\n",
    "    numerical_vars = [var for var in variables_applied_with_method if df_original[var].dtype != 'object']\n",
    "\n",
    "    print(\"\\n=====================================================================================\")\n",
    "    print(\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\\n\")\n",
    "    print(f\"{variables_applied_with_method}\\n\")\n",
    "\n",
    "    for var in categorical_vars:\n",
    "        df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
    "        df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
    "        df_combined = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        sns.countplot(data=df_combined, x=\"Value\", hue=\"Type\", palette=['#432371', \"#FAAE7B\"])\n",
    "        plt.title(f\"Distribution Plot {flag_count}: {var} (Categorical Variable)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend(title=\"Dataset\")\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "\n",
    "        flag_count += 1\n",
    "\n",
    "    for var in numerical_vars:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True, element=\"step\")\n",
    "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True, element=\"step\")\n",
    "        plt.title(f\"Distribution Plot {flag_count}: {var} (Numerical Variable)\")\n",
    "        plt.legend(title=\"Dataset\")\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "        flag_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the identified numerical variables with missing data for targeted imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['2ndFlrSF', 'BedroomAbvGr', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea', 'WoodDeckSF', 'EnclosedPorch']\n",
    "variables_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the Median Imputation method to handle missing data across the selected numerical variables. This replaces missing values with the median of the respective variable, ensuring robustness against outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "df_method = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Data Cleaning Effect function to visualize the impact of this method on the distribution of numerical variables, comparing the original and cleaned datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the categorical variables with missing data for targeted imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['GarageFinish', 'BsmtFinType1']\n",
    "variables_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a new category called 'Missing' to account for variables with missing data. This is distinct from None, as it is unclear whether the variable is missing due to omission or if it genuinely represents the absence of a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "imputer = CategoricalImputer(imputation_method='missing',fill_value='Missing',\n",
    "                             variables=variables_method)\n",
    "\n",
    "df_method = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaningEffect(df_original=df,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Drop Variables: Remove EnclosedPorch and WoodDeckSF from the dataset as they contain over 88% missing values, making them unsuitable for reliable analysis or imputation.\n",
    "\n",
    "2. Median Imputation: Apply the median imputation method to the following numerical variables with missing data: LotFrontage, GarageYrBlt, MasVnrArea, BedroomAbvGr, and 2ndFlrSF. This approach replaces missing values with the median of each respective variable, ensuring robustness against outliers.\n",
    "\n",
    "3. Categorical Imputation: Assign the category 'Missing' to fill missing values in the categorical variables GarageFinish and BsmtFinType1. This explicitly captures the absence of data without conflating it with meaningful categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into Train and Test sets to evaluate the effectiveness of our imputation methods. The imputation techniques are applied to the Train set, allowing the model to learn from the cleaned data. We then assess the performance and impact of these methods on the Test set to ensure generalisability and robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df['SalePrice'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = EvaluateMissingData(TrainSet)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dropping EnclosedPorch and WoodDeckSF as these have over 88% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['EnclosedPorch', 'WoodDeckSF' ]\n",
    "\n",
    "print(f\"* {len(variables_method)} variables to drop \\n\\n\"\n",
    "    f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "df_method = imputer.transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the selected variables have been successfully dropped by checking that they are no longer present in the transformed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_variables = set(variables_method).intersection(set(df_method.columns))\n",
    "\n",
    "if not remaining_variables:\n",
    "    print(\"Success: All specified variables have been dropped.\")\n",
    "else:\n",
    "    print(f\"Failure: These variables were not dropped: {remaining_variables}\")\n",
    "\n",
    "df_method.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply these changes to both Train and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_features_count = TrainSet.isnull().any().sum()\n",
    "print(f\"There are {missing_features_count} features with missing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Median Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now impute the median values for numerical variables with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'BedroomAbvGr', '2ndFlrSF' ]\n",
    "print(f\"* {len(variables_method)}  median\\n\\n\"\n",
    "    f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "df_method = imputer.transform(TrainSet)\n",
    "\n",
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Analysis\n",
    "General Observation:\n",
    "\n",
    "The missing values have been replaced with the median value for each feature. While this method is robust against outliers, it has introduced subtle changes in the distribution of some features, which could influence their predictive power.\n",
    "\n",
    "Feature: LotFrontage:<br>\n",
    "The imputed median value (~70) now accounts for a disproportionately large number of observations compared to the original distribution.\n",
    "This concentration of values around the median could reduce the feature's variability and potentially affect its predictive accuracy in capturing the true relationship with the target variable (SalePrice).\n",
    "\n",
    "Feature: GarageYrBlt:<br>\n",
    "While imputing the missing values in GarageYrBlt around the median year helps address the missing data, it may significantly impact the predictive power of this feature. Given that the age of the garage and property can strongly influence sale price, this approach might oversimplify the variability and dilute the feature's ability to capture its true relationship with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit transform the changes to the Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = MeanMedianImputer(imputation_method='median', variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the current missing data levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Categorical Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another category 'missing' to define the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = [ 'GarageFinish', 'BsmtFinType1' ]\n",
    "print(f\"* {len(variables_method)}  categorical\\n\\n\"\n",
    "    f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "imputer = CategoricalImputer(imputation_method='missing',fill_value='Missing', variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "df_method = imputer.transform(TrainSet)\n",
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another category of 'Missing' has been added to handle the missing data. This does not effect the categories already present in the feature. \n",
    "The feature engineering process will handle the values as numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate changes to the Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = CategoricalImputer(imputation_method='missing',fill_value='Missing', variables=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally check once more for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)\n",
    "EvaluateMissingData(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save cleaned train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltNetd085qHf"
   },
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKlnIozA4eQO",
    "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/cleaned')\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
